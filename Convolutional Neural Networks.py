# -*- coding: utf-8 -*-
"""Yifan Lou CMSC422 Problem Set 8

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GQyeqdd8GNpty76wGsmqOa8y5ZXXLUIf

# Problem Set 8: Convolutional Neural Networks
# CMSC 422, Fall 2020
# Due Dec 4 at 11:59pm

<center>

<img src="https://i.imgflip.com/4l0ruv.jpg" alt="meme" width="500px"/>
<br>
<img src="https://thescinder.files.wordpress.com/2017/06/goingtoneedagpuimgflip1.jpg" alt="meme" width="500px"/>
<br>
<img src="https://i.pinimg.com/originals/5a/cf/c2/5acfc296101e82b8dca3031fb3512121.png" alt="meme" width="500px"/>
<br>
<img src="https://twilio-cms-prod.s3.amazonaws.com/images/tfmlengineermeme.width-500.png" alt="meme" width="500px"/>
<br>
<img src="https://images.ctfassets.net/be04ylp8y0qc/1AYDDqzqoLNU7GyvMOLlTL/146ab0e8b96a25c1b3538a2929dae646/meme_6cce0f3bba63820b84f52ce05a822975_800.png?fm=jpg" alt="meme" width="500px"/>

</center>

# Instructions
In Problem Set 7 you had to implement neural networks from scratch. Now that you have a sense of what deep learning libraries do under the hood, you will implement convolutional neural networks (CNNs) using Keras. There might be some code provided for you here, and you will be required to write your implementations in places marked with __```#TODO: your code here```__. You may add helper functions if you feel you need to.  
__Note:__ you are expected to use the publicly available documentation of the python packages we'll be using.

__Analysis Questions:__ In addition to Python programming, each problem may contain some analysis questions. These are meant to ensure you understand your results, and will be manually graded on Gradescope.

__Submission:__ download this notebook as a `.ipynb` file and submit it to Gradescope. This assignment will be completely manually graded, but still, make sure to follow instructions closely.  

- Make sure your plots are visible when downloading the notebook, otherwise they won't appear on Gradescope. 
- Make sure your code cells are not throwing exceptions.
- Please do not import any packages other than what has already been imported here. You may be penalized for doing so. However, feel free to import specific sub-modules from packages already loaded for your convenience.

__THROUGHOUT THE NOTEBOOK: WHEN APPROPRIATE MAKE SURE TO PRINT / PLOT YOUR RESULTS AND THAT THEY ARE VISIBLE ON GRADESCOPE OR YOU WILL LOSE CREDIT (WE WILL NOT RUN YOUR CODE).__

# __Part 1: Computer Audition__

"Computer audition (CA) or machine listening is general field of study of algorithms and systems for audio understanding by [a] machine" (from Wikipedia).
In this part of the assignmnet you will implement a CNN in Keras to tackle a supervised classification task. Specifically, you will build a speech recognition network. A high-level diagram is displayed below.
<center>
<img src="https://drive.google.com/thumbnail?id=19RSc579LA7jefQmSw1qpdMJkDSywHAm6&sz=w1000" alt="block_cnn"/>
<i>Figure 1: High-level block diagram for part 1.</i>
</center>

## Getting Started with Audio

You might be wondering what the heck these orange/purplish heat maps are and what they have to do with audio. The answer is spectrograms and proven representation. When audio is recorded, physical sound waves (e.g. air being compressed and decompressed in a fraction of a second) are first converted into analog information, which in turn gets converted into digital sound waves, a.k.a. raw audio data which looks like this -  
<center>
<img src="https://drive.google.com/thumbnail?id=1erxL8OMi2pzYblMafPc2lgG_0JP5NeMi&sz=w300" alt="wave"/>
<br>
<i>Figure 2: An audio wave.</i>
</center>

This is just a long sequence of amplitude values, e.g. $[-0.67, -0.15, 0, 0.53, 0.86, ...]$. The number of amplitude values captured per second is called the ___sample rate___. The number of bits used to describe each sample (each value) is called the ___bit depth___. The higher the sample rate, the more frequencies we can capture; the higher the bit depth, the more volume levels we can capture.  
  
However, there are other ways to represent sound.
While raw audio is a time-domain representation, __spectrograms__ are a time-frequency representation, with the x-axis representing time and the y-axis representing frequency. The color represents the magnitude. Usually, the brighter the color, the louder the frequency. Note: they are presented with color, but this is simply to make for a more intelligible image, spectrograms are usually analyzed as a 2D array, which is essentially a grayscale image. 



<center>
<img src="https://drive.google.com/thumbnail?id=122TlTGZ04PcNCwggGcwfcMSt8Z7x_jLP&sz=w600" alt="wave"/>
<br>
<i>Figure 3: Spectrograms of the words "hot", "hat", "hit" & "head" spoken once with a high-pitched voice (top), and then again with a lower pitched voice (bottom). Taken from <a href="https://auditoryneuroscience.com/vocalizations-speech/formants-harmonics">auditoryneuroscience.com</a>.</i>
</center>

Spectrograms are often used for computer audition tasks as they generally offer a more effective representation. They make it easier for the model by essentially performing feature extraction, pulling out high level information. This information can then be used by CNNs, which perform well on image data.

Spectrograms are commonly generated by applying a transformation called "Discrete Fourier Transform" (DFT) to fixed-sized portions of the audio data. From Wikipedia: "Digitally sampled data, in the time domain, is broken up into chunks, which usually overlap, and Fourier transformed to calculate the magnitude of the frequency spectrum for each chunk. Each chunk then corresponds to a vertical line in the image; a measurement of magnitude versus frequency for a specific moment in time (the midpoint of the chunk). These spectrums or time plots are then 'laid side by side' to form the image." 

A __Short-time Fourier Transform__ (STFT) does just that, it applies a DFT to a fixed-size segment/window of the audio data, and slides that window along the data, such that we can see how the frequncy information changes over time. The entries of the 2D array generated by the STFT are actually complex numbers. The magnitude (or absolute value) of the STFT is called ___spectrogram___. You can play with [_this cool interactive visualization_](https://musiclab.chromeexperiments.com/Spectrogram/) to better understand spectrograms of different sounds.

## Import Needed Packages

We'll be using Colab's GPU for this one, so navigate to `Runtime>Change runtime type` and make sure you have GPU selected. Then, run the cell below to import the packages we'll be using.
"""

###########################
# DO NOT MODIFY THIS CELL #
###########################

!pip -q install librosa  # installing librosa
!pip3 -q install pydub

import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
import librosa, librosa.display  # python package for audio/music analysis
import matplotlib.pyplot as plt
import sklearn
import sys, os
from IPython.display import Audio
from IPython import display

"""## Obtaining the Dataset

We will use one of TensorFlow's publicly available datasets for audio processing: "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition." It contains one-second (or less) audio clips with the following contents: silence, an unknown word, "yes", "no", "up", "down", "left", "right", "on", "off", "stop", or "go". Thanks to the `tensorflow_datasets` package, we can download it with a single line of code. This dataset is over 8 GB so it'll take a while (took me around 15 minutes).

__Note:__ `tfds.load()` returns a `tf.data.Dataset` object, which is an abstraction over a dataset, and allows iteration to happen in a streaming fashion, so the full dataset does not need to fit into memory.

__Another Note__: once you've downloaded the data, re-running this cell won't re-download the dataset, it will simply use the downloaded data to re-populate the variables, which may come in handy. 
"""

###########################
# DO NOT MODIFY THIS CELL #
###########################

ds, ds_info = tfds.load('speech_commands', split=['train', 'validation', 'test'],
                        as_supervised=True, with_info=True)  # download the data
train_set, valid_set, test_set = ds  # get train-validation-test split
ds_info  # display dataset info

"""Let's listen to a few files from the dataset. Using the **Audio(filename)** function from IPython notebook, you can listen to an audio file directly in Colab. Bear in mind that the sample rate of the audio recordings is 16 kHz."""

sr = 16000  # sample rate
example = train_set.take(3)  # example is (audio, label)

for signal, label in tfds.as_numpy(example):
  print('signal = ', signal, '; label =', label)
  display.display(Audio(signal, rate=sr))

"""## Problem 1: Visualization (10 Points)

Now that we have our data, let's visualize and listen to one of the recordings.

### 1.1 Waveform (4 Points)
Plot one of the recordings' waveform with the title "Waveform" using the `librosa` package. Label the x-axis "Time (s)" and the y-axis "Amplitude". Display audio controls below so you could listen to the recording. Here is a link to the [official `librosa` documentation](https://librosa.org/doc/latest/index.html).
"""

# TODO: your code here
for s, l in tfds.as_numpy(train_set.take(1)):
  plt.figure()
  librosa.display.waveplot(np.array(s).astype(float), sr=16000)
  plt.title("Waveform")
  plt.xlabel("Time(s)")
  plt.ylabel("Amplitude")
  plt.show()

"""### 1.2 Spectrogram (3 Points)
Next, Let's plot the spectrogram of the same signal by applying the STFT using the librosa package and taking the absolute value of the result. Use matplotlib's `pcolormesh` function, you don't need to add labels here.
"""

# TODO: your code here
for s, l in tfds.as_numpy(train_set.take(1)):
  plt.figure()
  plt.pcolormesh(np.abs(librosa.stft(np.array(s).astype(float))))

"""### 1.3 Better Spectrogram (3 Points)
A little hard to notice the data right? let's scale the data and make the visualization more intelligible. Use librosa's amplitude to dB convertion function. Then, use Librosa's `specshow` function to display the spectrogram, label the x-axis "Time" and the y-axis "Hz". Lastly, add a dB colorbar and give the plot the title "Spectrogram".
"""

# TODO: your code here
for s, l in tfds.as_numpy(train_set.take(1)):
  temp = librosa.stft(np.array(s).astype(float))
  S_db = librosa.amplitude_to_db(np.abs(temp))
  plt.figure()
  librosa.display.specshow(S_db, sr=16000, x_axis='time', y_axis='linear')
  plt.title("Spectrogram")
  plt.xlabel("Time")
  plt.ylabel("Hz")
  plt.colorbar(format="%+2.f dB")
  plt.show()

"""## Problem 2: Preprocessing (10 Points)

Before we build our network, we need to perform some preprocessing to create a spectrogram dataset. An alternative you might see in practice is to make the input transformation a part of the network (either as a trainable or non-trainable layer), which would allow you to modify the transformation without having to process the dataset every time.

### 2.1 Filtering (3 Points)

Our model will be receiving fixed-sized inputs, so to keep things simple, we will simply filter out any recording which is not exactly 16,000 samples (equivalent to 1-second with a 16-kHz sample rate). Use TensorFlow's Dataset API. Do this to all sets (train, valid, and test).
"""

# filter the data to ensure consistent shape (removes ~8,000 samples)
# TODO: your code here
train_set_ft = train_set.filter(lambda signal, label : (len(signal) == 16000))
test_set_ft = test_set.filter(lambda signal, label : (len(signal) == 16000))
valid_set_ft = valid_set.filter(lambda signal, label : (len(signal) == 16000))

"""### 2.2 Transformation (4 Points)

Our model will be receiving spectrograms. Use TensorFlow's Dataset API, signal module, and other TensorFlow functions to transform the data into magnitue spectrograms. In particular, use a decibel-scaled spectrogram with 512 for your number of fast-fourier transforms (frame length) and a hop length (frame step) of 160. Implement `get_db_spec()` which receives a signal as input and returns the decibel-scaled spectrogram as output. Then, outside the function, transform the data using the function.  Do this to all sets (train, valid, and test). Use the power_to_db helper function (notice its docstring).
"""

def power_to_db(S, amin=1e-16, top_db=80.0):
    """Convert a power-spectrogram (magnitude squared) to decibel (dB) units.
    Computes the scaling ``10 * log10(S / max(S))`` in a numerically
    stable way. Based on:
    https://librosa.org/doc/latest/generated/librosa.power_to_db.html
    """
    def _tf_log10(x):
        numerator = tf.math.log(x)
        denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))
        return numerator / denominator
    
    # Scale magnitude relative to maximum value in S. Zeros in the output 
    # correspond to positions where S == ref.
    ref = tf.reduce_max(S)

    log_spec = 10.0 * _tf_log10(tf.maximum(amin, S))
    log_spec -= 10.0 * _tf_log10(tf.maximum(amin, ref))
    log_spec = tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)

    return log_spec

def get_db_spec(sig):
  # TODO: your code here
  mag_spec = tf.signal.stft(tf.cast(sig, tf.float32), frame_length = 512, frame_step = 160)
  mag = tf.abs(mag_spec)
  return power_to_db(mag**2)
  
# TODO: your code here
train_set_db = train_set_ft.map(lambda x, y : (tf.transpose(get_db_spec(x)), y))
test_set_db = test_set_ft.map(lambda x, y : (tf.transpose(get_db_spec(x)), y))
valid_set_db = valid_set_ft.map(lambda x, y : (tf.transpose(get_db_spec(x)), y))

#a = train_set_db.take(1)
#for s, l in tfds.as_numpy(a):
#  print(s)
#  print(tf.shape(s))
#  print(tf.shape(l))

"""### 2.3 Normalization (3 Points)

There are different ways to normalize data. For simplicity, let's perform per-sample standardization. Implement the `normalize()` function which recieves a `tf.Tensor` tensor and returns a standardized value by subtracting the mean of the tensor and divinding by the standard deviation plus some tiny value to avoid dividing by zero (use `TensorFlow` functions here). Then, apply it to the 3 subsets of the dataset (train, valid, and test).

"""

def normalize(x, epsilon=1e-10):
  # TODO: your code here
  m = tf.reduce_mean(x)
  sd = tf.math.reduce_std(x) + epsilon
  return tf.map_fn(fn = lambda t : tf.divide(tf.subtract(t, m), sd), elems = x)

# TODO: your code here
train_set_norm = train_set_db.map(lambda x, y : (normalize(x), y))
test_set_norm = test_set_db.map(lambda x, y : (normalize(x), y))
valid_set_norm = valid_set_db.map(lambda x, y : (normalize(x), y))

#a = train_set_norm.take(1)
#for s, l in tfds.as_numpy(a):
#  print(s)
#  print(tf.shape(s))
#  print(tf.shape(l))

"""## Problem 3: Architecture (15 Points)

Now we're ready to build our CNN model. You'll be implementing the model shown below. Use `tensorflow.keras` (either the sequential API or functional API) to implement and return the model inside the `my_cnn` function. 

Notice that the leftmost block is the reshaped input of shape 257x97x1 (CNN implementations usually require a 3rd dimension because of potential color channels). The convolutional layers are followed by batch normalization layers, which are followed by ReLU activations. Padding is always used such that all pixels are considered during the convolution _and_ pooling processes. Lastly, the output layer is a dense layer with a softmax activation. 

Don't forget to compile the model before returning it. Use the `adam` optimizer and for the loss function use sparse categorical crossentropy, the sparse part is to tell the model that our targets are __not__ one-hot encoded. Be sure to run the cell to see the model summary.

<center>
<img src="https://drive.google.com/thumbnail?id=1bAtVD-ffxyVi91kzfghXk3P0_rTDlCxf&sz=w3800" alt="cnn_diagram"/>
<i>Figure 4: Diagram of a deep convolutional neural network for you to implement.</i>
</center>
"""

# set random seeds for reproducibility
np.random.seed(42)
from tensorflow.random import set_seed
set_seed(42)
from tensorflow.keras import layers, models


input_shape = (257, 97)
        
def my_cnn(n_classes=12):
  model = tf.keras.Sequential(name = "CNN_Model")
  model.add(tf.keras.layers.Reshape((257, 97, 1), input_shape = (257, 97), name = "Reshape_Input"))
  #input_shape = model.output_shape
  #model.add(tf.keras.Input(shape = input_shape))

  model.add(tf.keras.layers.Conv2D(n_classes, (3,3), padding = 'same', name = "Conv_1"))
  model.add(tf.keras.layers.BatchNormalization(name = "Batch_Norm_1"))
  model.add(tf.keras.layers.ReLU(name = "ReLU_1"))
  model.add(tf.keras.layers.MaxPooling2D((3,3), strides = 2, padding = 'same',name = "MaxPool_1"))

  model.add(tf.keras.layers.Conv2D(n_classes * 2, (3,3), padding = 'same',name = "Conv_2"))
  model.add(tf.keras.layers.BatchNormalization(name = "Batch_Norm_2"))
  model.add(tf.keras.layers.ReLU(name = "ReLU_2"))
  model.add(tf.keras.layers.MaxPooling2D((3,3), strides = 2, padding = 'same',name = "MaxPool_2"))

  model.add(tf.keras.layers.Conv2D(n_classes * 4, (3,3), padding = 'same',name = "Conv_3"))
  model.add(tf.keras.layers.BatchNormalization(name = "Batch_Norm_3"))
  model.add(tf.keras.layers.ReLU(name = "ReLU_3"))
  model.add(tf.keras.layers.MaxPooling2D((3,3), strides = 2, padding = 'same',name = "MaxPool_3"))

  model.add(tf.keras.layers.Conv2D(n_classes * 4, (3,3), padding = 'same',name = "Conv_4"))
  model.add(tf.keras.layers.BatchNormalization(name = "Batch_Norm_4"))
  model.add(tf.keras.layers.ReLU(name = "ReLU_4"))

  model.add(tf.keras.layers.Conv2D(n_classes * 4, (3,3), padding = 'same',name = "Conv_5"))
  model.add(tf.keras.layers.BatchNormalization(name = "Batch_Norm_5"))
  model.add(tf.keras.layers.ReLU(name = "ReLU_5"))

  model.add(tf.keras.layers.GlobalMaxPooling2D(name = "Global_MaxPool"))
  model.add(tf.keras.layers.Dropout(0.2, name = "Dropout"))

  model.add(tf.keras.layers.Dense(n_classes * 4, name = "Dense"))
  model.add(tf.keras.layers.Dense(n_classes, activation="softmax", name = "Dense_SoftMax_1"))
  #model.add(tf.keras.layers.Dense(12, activation="softmax", name = "Dense_SoftMax_2"))

  model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'],
  ) 
  return model


model = my_cnn()
model.summary()

"""## Problem 4: Train & Test (25 Points)

Let's train and test our model.

### 4.1 Training (10 Points)

Using Colab's GPU, with batch size 128, each epoch took me ~5 minutes. I was able to reach 90% validation accuracy after a few epochs and 95% after a few more. Your resulting model must reach at least 95% to get full credit, or at least 90% for partial credit. 

Fit your network to the training data. As a tip, we suggest you checkpoint your model based on validation accuracy. Hint: you might not be able to pass the sets in their current form directly into the fit() function. Be sure to print the validation loss of your resulting model.
Then, plot the history of the training process with a curve describing the train loss, a curve describing the validation loss, a title of "History", y-axis labeled "Value", x-axis labeled "No. epoch", and a legend to specify which curve is which.
"""

# set random seeds for reproducibility
np.random.seed(42)
from tensorflow.random import set_seed
set_seed(42)
from keras.callbacks import ModelCheckpoint

# TODO: your code here (model training)
train_set_batch = train_set_norm.batch(128)
valid_set_batch = valid_set_norm.batch(128)

history = model.fit(
    train_set_batch,
    validation_data = valid_set_batch, 
    #batch_size = 128,
    epochs = 10,
    callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath = '/tmp/checkpoint', save_weights_only = False, 
                          monitor = 'val_accuracy', mode = 'max', save_best_only = True)],
)
# TODO: your code here (history plot)
metrics = history.history
plt.figure()
plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])
plt.legend(['loss', 'val_loss'])
plt.title("History")
plt.xlabel("No.epoch")
plt.xlim(0, 10)
plt.ylabel("Value")
plt.show()

"""### 4.2 Test (15 Points)
Let's see how our model performs on the test set. Use Keras' `evaluate` function to get the test loss and test accuracy, and plot the confusion matrix. Your test accuracy must be at least 90% to get full credit and the confusion matrix should have the features shown in the image below, namely the axis labels, ticks, values which are normalized (do not need to be the same values), and a color bar:
<center>
<img src="https://drive.google.com/thumbnail?id=12aSyDa53eBT-os4EJYFYGtwYXo6vy-ib&sz=w700" alt="wave"/>
<br>
</center>
"""

class_names = ['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up',
               'yes','unknown', 'silence']

# TODO: your code here (Keras evaluation)
test_set_batch = test_set_norm.batch(128)
result = model.evaluate(test_set_batch)
print("test loss, test acc:", result)

# TODO: your code here (confusion matrix)
from sklearn.metrics import confusion_matrix, plot_confusion_matrix
import pandas as pd
import seaborn as sns

test_signal = []
test_labels = []

for signal, label in test_set_norm:
  test_signal.append(signal.numpy())
  test_labels.append(label.numpy())

test_signal = np.array(test_signal)
test_labels = np.array(test_labels)

label_pred = np.argmax(model.predict(test_signal), axis=1)
label_true = test_labels
test_acc = sum(label_pred == label_true) / len(label_true)
print(f'Test set accuracy: {test_acc:.0%}')

confusion_mtx = confusion_matrix(y_true, y_pred, normalize='true') 
plt.figure(figsize=(10,10))
sns.heatmap(confusion_mtx, annot=True)
plt.xlabel('Predicted label')
plt.ylabel('True label')
xlocs, xlabels = plt.xticks() 
ylocs, ylabels = plt.yticks()
plt.xticks(xlocs, class_names, rotation='vertical')
plt.yticks(ylocs, class_names, rotation='horizontal')
plt.show()

"""### 4.3 Fun Test (Ungraded)

Let's use the Web Audio API to see if our model can predict correctly when listening to our own voice. Run the cell below to record a 1-second recording of yourself saying one of the target words (e.g. "up", "down", etc.). Listen to the result to ensure that the recording is good (try speaking right after "Recording..." appears). Then, run the next cell to see if your model predicted correctly.
"""

from IPython.display import Javascript
from google.colab import output
from base64 import b64decode

RECORD = """
const sleep  = time => new Promise(resolve => setTimeout(resolve, time))
const b2text = blob => new Promise(resolve => {
  const reader = new FileReader()
  reader.onloadend = e => resolve(e.srcElement.result)
  reader.readAsDataURL(blob)
})
var record = time => new Promise(async resolve => {
  stream = await navigator.mediaDevices.getUserMedia({ audio: true })
  recorder = new MediaRecorder(stream)
  chunks = []
  recorder.ondataavailable = e => chunks.push(e.data)
  recorder.start()
  await sleep(time)
  recorder.onstop = async ()=>{
    blob = new Blob(chunks)
    text = await b2text(blob)
    resolve(text)
  }
  recorder.stop()
})
"""

def record(sec=1.1):
  print('Recording...')
  display.display(Javascript(RECORD))
  s = output.eval_js('record(%d)' % (sec*1000))
  b = b64decode(s.split(',')[1])
  fp = 'my_rec.wav'
  with open(fp,'wb') as f:
    f.write(b)
  print('Done.')
  return fp

fp = record()
signal, sr = librosa.load(fp, sr=16000)
signal = signal[-sr:]
display.display(Audio(signal, rate=sr))

"""If your recording sounds reasonable, run the next cell to see if the model recognizes your speech!"""

# predict a single recording
spec = normalize(get_db_spec(signal))
one_hot_pred = model.predict(np.array([spec]))
pred_to_text = class_names[np.argmax(one_hot_pred)]
print('You said', pred_to_text)

"""# **Part 2: Computer Vision (Image Classification on CIFAR-10)**

In this section, we will explore implementing and improving upon some simple convolutional networks to perform image recognition on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. This dataset consists of $32 \times 32$ RGB images of objects from 10 classes: `airplane`, `automobile`, `bird`, `cat`, `deer`, `dog`, `frog`, `horse`, `ship`, and `truck`.

We'll be using Colab's GPU to expedite our training, so before we get started, navigate to `Runtime>Change runtime type` and make sure you have GPU selected.

<center>
<img src="https://drive.google.com/thumbnail?id=1wgoaxYIpKCq9Ihu7rfrursmsFgz318HN&sz=w600" alt="block_cnn"/>
<br>
<i>The CIFAR-10 Dataset: 10 random images from each class.</i>
</center>

### **Imports and Data Preparation**

Now, let's load and format the dataset, which is conveniently available directly from Keras, whilst importing any packages we might need.
"""

# DO NOT MODIFY THIS CELL
import numpy as np
import tensorflow as tf
from tensorflow import keras
import imgaug as ia
from imgaug import augmenters as iaa
import matplotlib
import matplotlib.pyplot as plt

classes = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog',
                    'frog', 'horse', 'ship', 'truck'])
(train, test) = keras.datasets.cifar10.load_data()

# convert uint8 RGB images to float RGB images in range [0,1]
train_input = train[0].astype(np.float32) / 255.0
test_input = test[0].astype(np.float32) / 255.0
# convert class labels to one-hot encoding
train_targets = keras.utils.to_categorical(train[1], num_classes=10)
test_targets = keras.utils.to_categorical(test[1], num_classes=10)

"""### **Problem 1: Building a Simple Model (5 Points)**

Let's start with a fairly barebones CNN architecture to tackle this learning problem, and dub it "model 1". The architecture details are displayed below.

$$
\begin{array}{c|c|c}
\text{Layer Type} & \text{Activation} & \text{Output Shape} \\
\hline
\text{Input} & \text{N/A} & (32,32,3) \\
\text{Conv2D} & \text{ReLU} & (32,32,8) \\
\text{MaxPool2D} & \text{N/A} & (16,16,8) \\
\text{Conv2D} & \text{ReLU} & (16,16,16) \\
\text{MaxPool2D} & \text{N/A} & (8,8,16) \\
\text{Flatten} & \text{N/A} & (1024,) \\
\text{Dense} & \text{Softmax*} & (10,)
\end{array}
$$

$\text{*}$ omit this activation; we'll use a more numerically stable training method that already has this in the loss function.

All convolutional filters have $3 \times 3$ spatial size, and all max pooling is done over a $2 \times 2$ grid in the spatial dimensions. We always use `same` padding to preserve the spatial dimensions. All trainable layers learn both weights and biases. The batch dimension has been omitted from the "$\text{Output Shape}$" column.

#### **1.1 Implementation (5 Points)**

Implement the above architecture using the Keras API in the function **`model_1`** provided below, which should simply create a Keras model and return it. For weight initialization, pick an appropriate initializer for the layer's activation function. Don't worry about compiling the model; we'll do that later.
"""

def model_1():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.InputLayer(input_shape=(32,32,3)))
  model.add(tf.keras.layers.Conv2D(8, (3,3), activation='relu', kernel_initializer='he_uniform', padding = 'same'))
  model.add(tf.keras.layers.MaxPool2D((2,2), padding = 'same'))
  model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu', kernel_initializer='he_uniform', padding = 'same'))
  model.add(tf.keras.layers.MaxPool2D((2,2), padding = 'same'))
  model.add(tf.keras.layers.Flatten())
  model.add(tf.keras.layers.Dense(10))
  
  return model

"""As a sanity check, let's visualize the implemented model, to confirm that it matches the description above (and that the model can be created without error). Note: the question mark you'll see in each shape, e.g. (?, 32, ...), is simply a placeholder for the amount of training data."""

# DO NOT MODIFY THIS CELL
keras.utils.plot_model(model_1(), show_shapes=True, show_layer_names=False)

"""### **Problem 2: Building a More Complex Model (5 Points)**

Next, let's also create a similar, but more complex model that we'll call "model 2". Its architecture is displayed below.

$$
\begin{array}{c|c|c}
\text{Layer Type} & \text{Activation} & \text{Output Shape} \\
\hline
\text{Input} & \text{N/A} & (32,32,3) \\
\text{Conv2D} & \text{ReLU} & (32,32,64) \\
\text{Conv2D} & \text{ReLU} & (32,32,64) \\
\text{MaxPool2D} & \text{N/A} & (16,16,64) \\
\text{Conv2D} & \text{ReLU} & (16,16,128) \\
\text{Conv2D} & \text{ReLU} & (16,16,128) \\
\text{MaxPool2D} & \text{N/A} & (8,8,128) \\
\text{Conv2D} & \text{ReLU} & (8,8,192) \\
\text{Conv2D} & \text{ReLU} & (8,8,192) \\
\text{MaxPool2D} & \text{N/A} & (4,4,192) \\
\text{Conv2D} & \text{ReLU} & (4,4,256) \\
\text{Conv2D} & \text{ReLU} & (4,4,256) \\
\text{MaxPool2D} & \text{N/A} & (2,2,256) \\
\text{Flatten} & \text{N/A} & (1024,) \\
\text{Dense} & \text{ReLU} & (512,) \\
\text{Dense} & \text{Softmax*} & (10,)
\end{array}
$$

$\text{*}$ as with model 1, omit this activation.

The filter and padding details for the layers are the same as in model 1.

#### **2.1 Implementation (5 Points)**

Just like you did with model 1, implement model 2 in the function **`model_2`** provided below.
"""

def model_2():
  model1 = tf.keras.Sequential()
  model1.add(tf.keras.layers.InputLayer(input_shape=(32,32,3)))
  model1.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.MaxPool2D((2,2), padding = 'same'))
  model1.add(tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.MaxPool2D((2,2), padding = 'same'))
  model1.add(tf.keras.layers.Conv2D(192, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.Conv2D(192, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.MaxPool2D((2,2), padding = 'same'))
  model1.add(tf.keras.layers.Conv2D(256, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.Conv2D(256, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.MaxPool2D((2,2), padding = 'same'))
  model1.add(tf.keras.layers.Flatten())
  model1.add(tf.keras.layers.Dense(512, activation='relu'))
  model1.add(tf.keras.layers.Dense(10))

  return model1

"""Again, let's visualize this one, too."""

# DO NOT MODIFY THIS CELL
keras.utils.plot_model(model_2(), show_shapes=True, show_layer_names=False)

"""### **Problem 3: Training experiments (10 Points)**

To get a sense of how these models perform on our dataset without taking much training time, let's see how the loss and accuracy of each model evolves during the first ten training epochs. Although each image in the dataset is small, there are 50,000 training examples, so this may take a minute or two.
"""

# DO NOT MODIFY THIS CELL
# get the models
m1 = model_1()
m2 = model_2()
hist = []
for (m_i, model) in enumerate([m1, m2]):
  opt = keras.optimizers.Nadam() # optimizer
  loss = keras.losses.CategoricalCrossentropy(from_logits=True) # loss function
  # compile and train, saving the training history (contains losses and metrics)
  model.compile(optimizer=opt, loss=loss, metrics=['categorical_accuracy'])
  hist.append(model.fit(x=train_input, y=train_targets,
                        validation_data=(test_input, test_targets),
                        batch_size=100, epochs=10, verbose=2))
# generate loss plots
fsize = plt.rcParams.get('figure.figsize') # the default figure size for scaling purposes
# create the plots side-by-side w/ locked y-axis for better comparison
num_plots = len(hist)
_, axarr = plt.subplots(1, num_plots, figsize=(fsize[0]*num_plots, fsize[1]), sharey=True)
for (m_i, hist_i) in enumerate(hist):
  axarr[m_i].semilogy(hist_i.history['loss'], label='model {:d} train loss'.format(m_i + 1))
  axarr[m_i].semilogy(hist_i.history['val_loss'], label='model {:d} test loss'.format(m_i + 1))
  axarr[m_i].set_xlabel('epoch')
  axarr[m_i].set_ylabel('loss')
  axarr[m_i].set_title('model {:d} loss'.format(m_i + 1))
  axarr[m_i].legend()
plt.tight_layout()
# generate accuracy plots (pretty much same procedure as with loss plots)
_, axarr = plt.subplots(1, num_plots, figsize=(fsize[0]*num_plots, fsize[1]), sharey=True)
for (m_i, hist_i) in enumerate(hist):
  axarr[m_i].plot(hist_i.history['categorical_accuracy'], label='model {:d} train acc'.format(m_i + 1))
  axarr[m_i].plot(hist_i.history['val_categorical_accuracy'], label='model {:d} test acc'.format(m_i + 1))
  axarr[m_i].set_xlabel('epoch')
  axarr[m_i].set_ylabel('accuracy')
  axarr[m_i].set_title('model {:d} accuracy'.format(m_i + 1))
  axarr[m_i].legend()
plt.tight_layout()

"""To get some numbers, let's also print out the best test accuracies for the models."""

# DO NOT MODIFY THIS CELL
print("best test acc:")
for (m_i, hist_i) in enumerate(hist):
  print("model {:d}: {:f}%".format(m_i + 1, max(hist_i.history['val_categorical_accuracy']) * 100.0))

"""#### **3.1 Analysis (10 Points)**

If you implemented the models correctly, your plots will look similar to the plots we got:

<center>
<img src="https://drive.google.com/thumbnail?id=10cAw3ftiegoWkPExynOrK_21uhZl68eF&sz=w800" alt="wave"/>
<br>
</center>

Also, here's the output we got when printing the best test accuracies for the models:
```
best test acc:
model 1: 62.760001%
model 2: 78.359997%
```

How well does each model fit to the training and test sets? How well do they perform relative to each other?  Why do you think our models behave this way?

***your answer here***
* Model 1 has best training accuracy of 0.6393 and testing accuracy of 0.6263, model 2 has best training accuracy of 0.9467 and testing accuracy of 0.7817. Model 2 perform better relatively than model 1, this is mainly because model 2 has way more layers than model 1, even with same parameters for everything else, number of layers can affect model accuracy by a lot.

### **Problem 4: Improving the model (20 Points)**

Based off of your observations in the above experiment, try to improve upon model 2 using some techniques discussed in the text and class.

#### **4.1 Implementation (10 Points)**

Write your code below implementing the modified model, training it, and displaying training plots similar to the ones above. A template based on the code above has been provided to help you get started, including code to train and plot the results, but feel free to modify or deviate from it as necessary.

As a guideline, try to get your improved model's accuracy on the test set either above $82\%$, or at least $5\%$ greater than the best accuracy the baseline model 2 gets above. Due to the stochastic nature of training, your results will vary with every run; it suffices that the result you submit meets at least one of these criteria.
"""

def model_2_improved():
  model1 = tf.keras.Sequential()
  model1.add(tf.keras.layers.InputLayer(input_shape=(32,32,3)))
  model1.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.BatchNormalization())
  model1.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.BatchNormalization())
  model1.add(tf.keras.layers.MaxPool2D((2,2), padding = 'same'))
  model1.add(tf.keras.layers.Dropout(0.2))
  model1.add(tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.BatchNormalization())
  model1.add(tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.BatchNormalization())
  model1.add(tf.keras.layers.MaxPool2D((2,2), padding = 'same'))
  model1.add(tf.keras.layers.Dropout(0.2))
  model1.add(tf.keras.layers.Conv2D(192, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.BatchNormalization())
  model1.add(tf.keras.layers.Conv2D(192, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.BatchNormalization())
  model1.add(tf.keras.layers.MaxPool2D((2,2), padding = 'same'))
  model1.add(tf.keras.layers.Dropout(0.2))
  model1.add(tf.keras.layers.Conv2D(256, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.BatchNormalization())
  model1.add(tf.keras.layers.Conv2D(256, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))
  model1.add(tf.keras.layers.BatchNormalization())
  model1.add(tf.keras.layers.MaxPool2D((2,2), padding = 'same'))
  model1.add(tf.keras.layers.Dropout(0.2))
  model1.add(tf.keras.layers.Flatten())
  model1.add(tf.keras.layers.Dense(512, activation='relu'))
  model1.add(tf.keras.layers.Dense(10))

  return model1

# Provided Code
model = model_2_improved() # get model
opt = keras.optimizers.Nadam() # optimizer
loss = keras.losses.CategoricalCrossentropy(from_logits=True) # loss function
# compile and train, saving history for use in plotting our losses and metrics
model.compile(optimizer=opt, loss=loss, metrics=['categorical_accuracy'])
improved_hist = model.fit(x=train_input, y=train_targets,
                 validation_data=(test_input, test_targets),
                 batch_size=100, epochs=30, verbose=2)
# plot losses
plt.figure()
plt.semilogy(improved_hist.history['loss'], label='train loss')
plt.semilogy(improved_hist.history['val_loss'], label='test loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.tight_layout()
# plot accuracies
plt.figure()
plt.plot(improved_hist.history['categorical_accuracy'], label='train acc')
plt.plot(improved_hist.history['val_categorical_accuracy'], label='test acc')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()
# print best
print("best test acc: {:f}%".format(max(improved_hist.history['val_categorical_accuracy']) * 100.0))

keras.utils.plot_model(model_2_improved(), show_shapes=True, show_layer_names=False)

"""#### **4.2 Dicussion and Analysis (10 Points)**

Provide some writing below describing what modifications you made to model 2's architecture and/or training procedure, as well as your rationale behind these modifications. How well does your improved model perform?

***your answer here***
* The modification I made was to add a BatchNormalization layer after every Conv2D layer. I just simply followed the architecture that was in Part 1 Problem 3, since this layer can normalize the activations of the previous layer at each batch. The model did improve with epoch num = 30, and the best accuracy of this model is 85.21%.
"""